{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edc3d1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# azure-data-lake-sentinel/src/data_processing_scripts/synapse_notebooks/text_extraction_chunking.ipynb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This is a conceptual Synapse Spark Notebook.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# --- Parameters ---\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# These parameters would be passed when triggering the Synapse Notebook via API/SDK\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdbutils\u001b[49m\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilePath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile Path to Process (ADLS Gen2)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputFolder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/processed_chunks/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Folder for Chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilePath\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# azure-data-lake-sentinel/src/data_processing_scripts/synapse_notebooks/text_extraction_chunking.ipynb\n",
    "\n",
    "# This is a conceptual Synapse Spark Notebook.\n",
    "# It would be triggered by the 'classification-orchestrator' Azure Function\n",
    "# for larger/complex files that require distributed processing for text extraction.\n",
    "\n",
    "# --- Parameters ---\n",
    "# These parameters would be passed when triggering the Synapse Notebook via API/SDK\n",
    "dbutils.widgets.text(\"filePath\", \"\", \"File Path to Process (ADLS Gen2)\")\n",
    "dbutils.widgets.text(\"outputFolder\", \"/processed_chunks/\", \"Output Folder for Chunks\")\n",
    "file_path = dbutils.widgets.get(\"filePath\")\n",
    "output_folder = dbutils.widgets.get(\"outputFolder\")\n",
    "\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# --- 1. Read Data from ADLS Gen2 ---\n",
    "# Spark can read directly from ADLS Gen2\n",
    "# Example: reading a CSV (adjust for PDF, Docx, etc.)\n",
    "# For binary files like PDF/Docx, you'd integrate with libraries like Apache Tika (if Spark supports it)\n",
    "# or external OCR services before this step.\n",
    "try:\n",
    "    if file_path.endswith(\".csv\"):\n",
    "        df_raw = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        # Combine relevant columns into a single text string for classification\n",
    "        df_text = df_raw.withColumn(\"full_text\", F.concat_ws(\" \", *df_raw.columns)).select(\"full_text\")\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        df_text = spark.read.text(file_path).withColumnRenamed(\"value\", \"full_text\")\n",
    "    elif file_path.endswith(\".json\"):\n",
    "        df_raw = spark.read.json(file_path)\n",
    "        # Flatten JSON and convert to text\n",
    "        df_text = df_raw.select(F.to_json(F.struct(F.col(\"*\"))).alias(\"full_text\"))\n",
    "    # Add more file types (PDF, Docx - require external libraries/services for content extraction)\n",
    "    else:\n",
    "        print(f\"Unsupported file type for direct text extraction: {file_path}\")\n",
    "        # Here, you might log an error and/or trigger an OCR service if it's an image.\n",
    "        # For this example, we'll just use the file path as text if content cannot be read.\n",
    "        df_text = spark.createDataFrame([(f\"File content could not be extracted directly from {file_path}.\")]).toDF(\"full_text\")\n",
    "\n",
    "    print(f\"Extracted {df_text.count()} rows of text.\")\n",
    "    df_text.show(truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file {file_path}: {e}\")\n",
    "    # Handle error: log, potentially send to a dead-letter queue.\n",
    "    df_text = spark.createDataFrame([(f\"Error processing {file_path}: {e}\")]).toDF(\"full_text\")\n",
    "\n",
    "\n",
    "# --- 2. Text Chunking (for large documents) ---\n",
    "# If a document is too large for a single API call to Azure OpenAI/Cognitive Services, chunk it.\n",
    "# This is a simplified example; real chunking requires more advanced NLP techniques (sentence tokenization etc.)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "MAX_CHUNK_SIZE = 4000 # Max characters for OpenAI/Cognitive Services input\n",
    "\n",
    "@F.udf(returnType=F.ArrayType(StringType()))\n",
    "def chunk_text_udf(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), MAX_CHUNK_SIZE):\n",
    "        chunks.append(text[i:i + MAX_CHUNK_SIZE])\n",
    "    return chunks\n",
    "\n",
    "df_chunks = df_text.withColumn(\"chunks\", chunk_text_udf(F.col(\"full_text\"))) \\\n",
    "                   .withColumn(\"chunk\", F.explode(\"chunks\")) \\\n",
    "                   .withColumn(\"original_file_path\", F.lit(file_path)) \\\n",
    "                   .select(\"original_file_path\", \"chunk\")\n",
    "\n",
    "print(f\"Generated {df_chunks.count()} chunks.\")\n",
    "df_chunks.show(truncate=False)\n",
    "\n",
    "# --- 3. Save Chunks to ADLS Gen2 ---\n",
    "# These chunks can then be processed in parallel by Azure Functions calling AI services.\n",
    "output_path = f\"abfss://{output_folder.strip('/')}@${data.azurerm_storage_account.adls_gen2_account.name}.dfs.core.windows.net/processed_chunks/\"\n",
    "df_chunks.write.mode(\"overwrite\").parquet(output_path) # Save as Parquet\n",
    "print(f\"Processed chunks saved to: {output_path}\")\n",
    "\n",
    "# Note: The Synapse Notebook would then signal completion (e.g., by writing a success file,\n",
    "# or the orchestrating Azure Function would poll for its status).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
