{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure-data-lake-sentinel/src/ml_models/sensitive_data_classifier/notebooks/custom_nlp_training.ipynb\n",
    "\n",
    "# This is a conceptual Jupyter Notebook for Azure ML experimentation.\n",
    "# You would run this on an Azure ML Compute Instance or Compute Cluster.\n",
    "\n",
    "# --- 1. Setup Azure ML Workspace ---\n",
    "import azure.ai.ml as ml\n",
    "from azure.ai.ml.entities import Data, Environment, Code, Compute, Job, AmlCompute, OnlineEndpoint, OnlineDeployment, Model\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Authenticate and get MLClient\n",
    "# Ensure your environment has AZUREML_ARM_SUBSCRIPTION, AZUREML_ARM_RESOURCEGROUP,\n",
    "# and AZUREML_ARM_WORKSPACE_NAME set (typically done automatically by AML compute).\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = ml.MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=os.environ.get(\"AZUREML_ARM_SUBSCRIPTION\"),\n",
    "    resource_group_name=os.environ.get(\"AZUREML_ARM_RESOURCEGROUP\"),\n",
    "    workspace_name=os.environ.get(\"AZUREML_ARM_WORKSPACE_NAME\")\n",
    ")\n",
    "\n",
    "print(f\"Connected to Azure ML Workspace: {ml_client.workspace_name}\")\n",
    "\n",
    "# --- 2. Register Data ---\n",
    "# This assumes you have your 'training_data.csv' ready in your ADLS Gen2\n",
    "# or uploaded to the default blob store of your AML workspace.\n",
    "# For simplicity, let's assume it's in a path relative to the workspace's default datastore.\n",
    "# You might need to manually upload a dummy training_data.csv for testing.\n",
    "# Example: training_data.csv with columns 'text' and 'label' (e.g., 'PII', 'Public')\n",
    "# text,label\n",
    "# \"John Doe, 123 Main St, Anytown, 12345\",PII\n",
    "# \"This is a public document about cloud computing.\",Public\n",
    "# \"Meeting minutes from Q4 strategy session. Highly confidential.\",Confidential\n",
    "\n",
    "data_path = \"azureml://datastores/workspaceblobstore/paths/data/sensitive_data_training/\"\n",
    "my_data = Data(\n",
    "    name=\"sensitive-data-training-data\",\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Training data for sensitive data classification.\",\n",
    "    tags={\"format\": \"csv\", \"sensitive\": \"false\"} # Tag as non-sensitive for the training data itself\n",
    ")\n",
    "# Uncomment to create/update the data asset\n",
    "# data_asset = ml_client.data.create_or_update(my_data)\n",
    "# print(f\"Data asset URI: {data_asset.path}\")\n",
    "# For demonstration, let's assume the data asset is named 'sensitive-data-training-data'\n",
    "# after manual upload or a previous run.\n",
    "data_asset_name = \"sensitive-data-training-data\"\n",
    "\n",
    "\n",
    "# --- 3. Create Environment ---\n",
    "# Define the custom environment using the conda_env.yml file\n",
    "custom_env_name = \"sensitive-data-classifier-env\"\n",
    "my_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for sensitive data classifier.\",\n",
    "    conda_file=os.path.join('..', 'conda_env.yml'), # Path to your conda_env.yml in the same repo\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\" # Base image\n",
    ")\n",
    "# Uncomment to create/update the environment\n",
    "# ml_client.environments.create_or_update(my_env)\n",
    "# print(f\"Environment '{custom_env_name}' created/updated.\")\n",
    "\n",
    "\n",
    "# --- 4. Create Compute Cluster (if not already provisioned by Terraform) ---\n",
    "# Ensure your AML Compute Cluster is attached to the VNet (ml-subnet)\n",
    "compute_name = \"ml-compute-cluster\"\n",
    "try:\n",
    "    compute = ml_client.compute.get(compute_name)\n",
    "    print(f\"Compute '{compute_name}' already exists.\")\n",
    "except Exception:\n",
    "    print(f\"Creating compute '{compute_name}'...\")\n",
    "    compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_DS3_v2\", # Matches your Terraform VM size\n",
    "        min_instances=0,\n",
    "        max_instances=4,\n",
    "        idle_time_before_scale_down=1800, # 30 minutes\n",
    "        # network_settings={ # Uncomment if your Terraform doesn't handle VNet for compute\n",
    "        #     \"vnet_name\": \"<YOUR_VNET_NAME>\",\n",
    "        #     \"subnet\": \"<YOUR_ML_SUBNET_NAME>\"\n",
    "        # }\n",
    "    )\n",
    "    # Uncomment to create/update compute\n",
    "    # ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    print(f\"Compute '{compute_name}' created/updated.\")\n",
    "\n",
    "\n",
    "# --- 5. Create Training Job ---\n",
    "# Define the command job that runs your train.py script\n",
    "code_folder = os.path.join('..') # Points to the folder containing train.py and conda_env.yml\n",
    "job = ml.command(\n",
    "    name=\"sensitive-data-classifier-training\",\n",
    "    display_name=\"Train Sensitive Data Classifier\",\n",
    "    description=\"Trains an NLP model to classify sensitive data in text.\",\n",
    "    inputs={\n",
    "        \"data_path\": ml.Input(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            path=f\"azureml:{data_asset_name}:latest\", # Use the registered data asset\n",
    "            mode=InputOutputModes.RO_MOUNT # Read-only mount\n",
    "        )\n",
    "    },\n",
    "    code=code_folder,\n",
    "    command=\"python train.py --data_path ${{inputs.data_path}} --model_output_path ${{outputs.model_output}}\",\n",
    "    environment=f\"{custom_env_name}@latest\", # Use the custom environment created above\n",
    "    compute=compute_name,\n",
    "    outputs={\n",
    "        \"model_output\": ml.Output(type=AssetTypes.MLFLOW_MODEL) # Save model in MLflow format (recommended)\n",
    "    },\n",
    "    experiment_name=\"sensitive-data-classification\"\n",
    ")\n",
    "\n",
    "# --- 6. Submit Training Job ---\n",
    "print(\"Submitting training job...\")\n",
    "# Uncomment to submit the job\n",
    "# returned_job = ml_client.jobs.create_or_update(job)\n",
    "# print(f\"Job ID: {returned_job.id}\")\n",
    "# ml_client.jobs.stream(returned_job.id) # Stream logs in real-time\n",
    "# returned_job.wait_for_completion()\n",
    "# print(f\"Training job completed with status: {returned_job.status}\")\n",
    "\n",
    "# For demonstration, let's assume a job completes and we get its ID\n",
    "# Replace with actual job ID from a successful run\n",
    "# If running interactively, uncomment the job submission above\n",
    "job_id = \"your-completed-training-job-id\" # e.g., \"sensitive-data-classifier-training_12345678\"\n",
    "\n",
    "\n",
    "# --- 7. Register Model (after job completion) ---\n",
    "print(\"Registering model...\")\n",
    "model_path_from_job = f\"azureml://jobs/{job_id}/outputs/model_output\"\n",
    "registered_model = ml_client.models.create_or_update(\n",
    "    Model(\n",
    "        name=\"sensitive-data-classifier-model\",\n",
    "        path=model_path_from_job,\n",
    "        type=AssetTypes.MLFLOW_MODEL, # Or AssetTypes.CUSTOM_MODEL if not MLflow compatible\n",
    "        description=\"NLP model for classifying PII, PCI, PHI, Confidential data.\",\n",
    "        tags={\"task\": \"text-classification\", \"sensitivity\": \"high\"}\n",
    "    )\n",
    ")\n",
    "print(f\"Model '{registered_model.name}' registered with ID: {registered_model.id}, Version: {registered_model.version}\")\n",
    "\n",
    "\n",
    "# --- 8. Deploy Endpoint (Real-time Online Endpoint) ---\n",
    "# Create an Azure ML Online Endpoint for real-time inference.\n",
    "endpoint_name = \"sensitive-data-classifier-endpoint\"\n",
    "endpoint = OnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Real-time endpoint for sensitive data classification\",\n",
    "    auth_mode=\"key\", # or \"aml_token\" - depends on how you authenticate\n",
    ")\n",
    "print(f\"Creating/updating endpoint '{endpoint.name}'...\")\n",
    "# ml_client.online_endpoints.begin_create_or_update(endpoint).wait()\n",
    "# print(f\"Endpoint '{endpoint.name}' is ready.\")\n",
    "\n",
    "\n",
    "# --- 9. Deploy Deployment to Endpoint ---\n",
    "# Create a deployment for the registered model on the endpoint.\n",
    "deployment_name = \"v1-deployment\" # A name for this specific deployment version\n",
    "deployment = OnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=registered_model, # The model registered previously\n",
    "    environment=f\"{custom_env_name}@latest\", # Use the custom environment\n",
    "    instance_type=\"Standard_DS3_v2\", # Matches your Terraform instance type for inference\n",
    "    instance_count=1,\n",
    ")\n",
    "print(f\"Creating/updating deployment '{deployment.name}' for endpoint '{endpoint.name}'...\")\n",
    "# ml_client.online_deployments.begin_create_or_update(deployment).wait()\n",
    "# print(f\"Deployment '{deployment.name}' is ready.\")\n",
    "\n",
    "\n",
    "# --- 10. Test Inference (Optional) ---\n",
    "# Assuming the endpoint is deployed and healthy\n",
    "# You would use the endpoint URL and key (from AML workspace) in your Azure Function\n",
    "# sample_text = {\"input_data\": {\"text\": \"My name is Alice and my SSN is 999-88-7777 and I live at 123 Pine St.\"}}\n",
    "# test_result = ml_client.online_endpoints.invoke(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     request_file=None, # Pass dictionary directly\n",
    "#     input=json.dumps(sample_text)\n",
    "# )\n",
    "# print(f\"Test inference result: {test_result}\")\n",
    "\n",
    "# --- 11. Cleanup (Optional, for temporary resources) ---\n",
    "# ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()\n",
    "# ml_client.compute.begin_delete(name=compute_name).wait()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
